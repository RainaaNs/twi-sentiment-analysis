{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a876618b-a07b-49e2-83d6-bd2485f5472b",
   "metadata": {},
   "source": [
    "### Combine all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56c979eb-ce33-4d5c-892d-4c5f274e5a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/Original datasets\\afrisenti_twi_dev.csv with UTF-8\n",
      "Loaded C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/Original datasets\\afrisenti_twi_test.csv with UTF-8\n",
      "Loaded C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/Original datasets\\afrisenti_twi_train.csv with UTF-8\n",
      "Encoding error in C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/Original datasets\\labeledl_twi_tweets.csv: 'utf-8' codec can't decode byte 0x92 in position 17: invalid start byte\n",
      "Loaded C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/Original datasets\\labeledl_twi_tweets.csv with cp1252 fallback\n",
      "Combined dataset saved in UTF-8: (10814, 2)\n"
     ]
    }
   ],
   "source": [
    "# if they end in csv\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "folder = \"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/datasets/original_data\"\n",
    "all_files = glob.glob(folder + \"/*.csv\")\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for f in all_files:\n",
    "    try:\n",
    "        temp = pd.read_csv(f, encoding=\"utf-8\")  \n",
    "        \n",
    "        print(f\"Loaded {f} with UTF-8\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Encoding error in {f}: {e}\")\n",
    "        temp = pd.read_csv(f, encoding=\"cp1252\")\n",
    "        print(f\"Loaded {f} with cp1252 fallback\")\n",
    "    df_list.append(temp)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df.to_csv(folder + \"/combined.csv\", index=False, encoding=\"utf-8\") \n",
    "print(\"Combined dataset saved in UTF-8:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04f53a7e-4e78-4101-b754-1dbcacb74326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "Positive               3542\n",
      "positive               2277\n",
      "negative               1815\n",
      "Neutral                1308\n",
      "Negative               1064\n",
      "neutral                 726\n",
      "No Sentiment             49\n",
      "Positive & Negative      26\n",
      "Multilingual              3\n",
      "Twi                       3\n",
      "Ghanaian Pidgin           1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Printing the count of all labels\n",
    "\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34377087-eb86-402f-a72f-bcae62f88229",
   "metadata": {},
   "source": [
    "### Remove all broken text from the dataset x  Normalise dataset by replacing 3s, ?s and ) with ɛ and ɔ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "993d6466-8cd3-4f1c-8b2e-11e523f61520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to combined_1.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from ftfy import fix_text\n",
    "\n",
    "def fix_broken(text):\n",
    "    return fix_text(text)\n",
    "\n",
    "def normalize_twi(text):\n",
    "    text = text.replace(\"3\", \"ɛ\")   # map 3 → ɛ\n",
    "    text = text.replace(\")\", \"ɔ\")   # map ) → ɔ\n",
    "    text = text.replace(\"ɛɛ\", \"ɛ\")  # clean duplicates\n",
    "    text = text.replace(\"?y?\", \"ɛyɛ\")  # common corruption\n",
    "    return text\n",
    "\n",
    "def restore_twi_chars(text):\n",
    "    text = re.sub(r\"\\?y\\?\", \"ɛyɛ\", text)\n",
    "    text = re.sub(r\"\\?k\\?\", \"ɔkɔ\", text)\n",
    "\n",
    "    start_map = {\n",
    "        \"b\": \"ɛb\",\n",
    "        \"d\": \"ɔd\",\n",
    "        \"m\": \"ɔm\",\n",
    "        \"n\": \"ɛn\",\n",
    "        \"k\": \"ɛk\",\n",
    "        \"t\": \"ɛt\",\n",
    "        \"s\": \"ɛs\",\n",
    "        \"w\": \"ɔw\",\n",
    "    }\n",
    "\n",
    "    for c, rep in start_map.items():\n",
    "        text = re.sub(rf\"\\?{c}\", rep, text)\n",
    "\n",
    "    end_map = {\n",
    "        \"m\": \"mɛ\",\n",
    "        \"n\": \"nɛ\",\n",
    "        \"t\": \"tɔ\",\n",
    "        \"b\": \"bɔ\",\n",
    "        \"w\": \"wɔ\",\n",
    "        \"s\": \"sɛ\",\n",
    "        \"k\": \"kɔ\",\n",
    "        \"f\": \"fɔ\",\n",
    "        \"h\": \"hɔ\",\n",
    "        \"p\": \"pɔ\",\n",
    "        \"r\": \"rɛ\",\n",
    "        \"e\": \"eɛ\",\n",
    "        \"a\": \"aɛ\",\n",
    "    }\n",
    "\n",
    "    for c, rep in end_map.items():\n",
    "        text = re.sub(rf\"{c}\\?\", rep, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "combined = pd.read_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/datasets/combined.csv\")\n",
    "\n",
    "combined[\"tweet\"] = (combined[\"tweet\"]\n",
    "                     .apply(fix_broken)\n",
    "                     .apply(normalize_twi)\n",
    "                     .apply(restore_twi_chars))\n",
    "\n",
    "combined[\"label\"] = combined[\"label\"].str.lower()\n",
    "\n",
    "combined.to_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/datasets/combined_1.csv\", index=False)\n",
    "\n",
    "print(\"Saved to combined_1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c43899-8735-4cf8-9faf-e94f9e486c0a",
   "metadata": {},
   "source": [
    "### Increase dataset size via augmentation by a target value, such that all labels have similar values (can be set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c24e4cd-fd6b-4531-a5dd-8e89f1bcbe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation complete: 10814 → 17302 rows\n"
     ]
    }
   ],
   "source": [
    "lexicon = {\n",
    "    \"neutral\": {\n",
    "        \"na wo dea den\": [\"me ho yɛ den\", \"wo ho ntɔ wo\", \"wo ho yɛ hu\"],\n",
    "        \"gyimie saaaa na afi aso\": [\"gyimie saaaa\", \"yɛ ɔkwasea saaaa na afi aso\", \"yɛ ɔkwasea saaa\"],\n",
    "        \"gyaii nsem hunuu no na bu oman no yiee\": [\"gyae nkwaseasɛm no na mo ne ɔman no nni yie\", \"gyae nkwaseasɛm no\", \"gyaii nsem hunuu no na bu ɔman\"],\n",
    "        \"covid19 nkoa mponi na ebola abaframu the game enter different level\": [\"covid19 nkoa na seesei ebola abaframu the game enter level eii\", \"covid19 nko ara na seesei ebola aba afra mu\", \"covid19 yɛ den oo\"],\n",
    "        \"is full example of fear of man and stop saman\": [\"yɛ onipa suro ho nhwɛso a edi mu na ɛnyɛ saman\", \"is example of onipa suro na ɛnyɛ ahonhommɔne\", \"onipa ho suro oo\"],\n",
    "        \"aden wo yɛ akwadaa\": [\"nea enti a woyɛ abofra\", \"aden enti a woyɛ abofra\", \"opanyin te sɛ ɔno\"],\n",
    "        \"dabi me dii bɛgo lol\": [\"dabi me dii paanoo lol\", \"dabii me di rice\", \"dabi me di fufu lol\"],\n",
    "        \"e choke u anaa\": [\"me ho yɛ bɔne\", \"me ho ntɔ me\", \"me ho yɛ hu\"],\n",
    "        \"nika no da me ha nso wall no\": [\"nka no da me ha nso wall no\", \"me nkutoo na mewɔ biribi a emu yɛ duru\", \"nanso ɛda ha ne me nso ɔfasuo no\"],\n",
    "        \"twam wogoro twer ne\": [\"twam wɔgoro twɛ ne\", \"pass wɔn di agorɔ twɛn na\", \"nanso wodi agorɔ na wɔtwɛn\"],\n",
    "        \"kraman mullion eii\": [\"kraman million eii\", \"ɔpepem akraman eii\", \"ɔpepem akraman eii\"],\n",
    "        \"dabi wasori\": [\"me ho yɛ bɔne\", \"me ho ntɔ me\", \"me ho yɛ hu\"],\n",
    "        \"akakasuro ne sorowisa\": [\"akakasuro ne soro wisa\", \"ehu ne ɔsoro home\", \"ehu ne sorowisa\"],\n",
    "        \"ankasa nasu yɛ din\": [\"nasu ankasa no yɛ din\", \"nasu yɛ din\", \"kofi yɛ din a wɔde frɛ no daa\"],\n",
    "        \"nyɛ saa na merep akyer\": [\"ɛnyɛ saa na merep akyer\", \"ɛnyɛ ɛno ne nea merebɔ mmɔden sɛ mɛka\", \"ɛno ne nea merebɔ mmɔden sɛ mɛka\"],\n",
    "        \"natsecurity foc nso se sn\": [\"me ho yɛ bɔne\", \"me ho ntɔ me\", \"me ho yɛ hu\"],\n",
    "        \"eeeii saa mennim da ??\": [\"eeeii enti na mennim da ??\", \"enti na minnim da\", \"enti efi bere bɛn?\"],\n",
    "        \"oh saa??\": [\"oh saa na ɛte??\", \"saa na ɛte\", \"oo saa\"],\n",
    "        \"shocki me mpo??\": [\"shocking me mpo??\", \"ɛyɛ me ahodwiriw mpo\", \"mpo nie\"],\n",
    "    },\n",
    "    \"negative\": {\n",
    "        \"kwasia sem saa na wabodam\": [\"kwasiasɛm saa na wa bo dam\", \"saa ɔkwasea na ɔyɛ kraman\", \"kwasiasɛm saa na wa gyimi\"],\n",
    "        \"wo brofo nso nyɛ d ??\": [\"wo borofo nso nyɛ d ??\", \"wo english nso nyɛ papa koraa\", \"wo brofo nso nyɛ papa\"],\n",
    "        \"konkonsani tena kurom a kuro bɔ\": [\"kɔnkɔnsani tena kurom bi?\", \"konkonsa bi te kuro yi mu nso?\", \"nsɛmmɔnedi wɔ baabiara\"],\n",
    "        \"wonni sika nanso wopɛ mmaa yɛde anomdwa tu tonga\": [\"wonni sika nanso wopɛ mmea de wɔn ano tow tonga\", \"wonni sika nanso hwɛ sɛ wɔrekasa\", \"wonni sika nanso hwɛ wɔn\"],\n",
    "        \"anisohye saa na wafa mu ewu\": [\"aniwa saa na wafa so awu\", \"enti na wafa so awu\", \"ɔnam so awu\"],\n",
    "        \"saa ppp nsemhunu nkoaa\": [\"saa paa nsemhunu nkoaa\", \"enti nkwaseasɛm ankasa nkutoo\", \"enti nkwaseasɛm nkutoo\"],\n",
    "        \"koko nkoaa aden wondidi biom anaa\": [\"koko nkoaa aden wondidi biom anaa\", \"koko nkoaa aden wondidi\", \"coconut nkoaa nea enti a wonnni bio anaa\"],\n",
    "        \"masa wo yare anaa pt\": [\"masa, wo yare anaa\", \"woyare anaa?\", \"so woafi w’adwene mu?\"],\n",
    "        \"mekyiri dɔkono ne nkyenam\": [\"me tan dɔkono ne nkyene\", \"me tan evil ne mpataa a wɔayam\", \"me tan kyenam\"],\n",
    "        \"wo yɛ aboa paa ɔɔɔɔɔɔɔɔɔɔ wo saa akoa wei??\": [\"wo yɛ aboa paa ɔɔɔɔɔɔɔɔɔɔ wo saa akoa no??\", \"wobetumi aboa wo saa akoa no??\", \"wo nkoa\"],\n",
    "        \"sia nanka yɛnni pilolo anaa\": [\"sia nanso yɛnni pilolo anaa\", \"nanso yɛnni pilolo sia\", \"nanka yɛnni pilolo sia\"],\n",
    "        \"i shock sef mo na mo bɛ tschew koraaaa\": [\"me shock me ho sɛ wo bɛ tschew koraa\", \"me shock me ho sɛ wo tschew koraa\", \"wo bɛ tschew\"],\n",
    "        \"born one ɛna wu kyere wu hu saa nu eii eugenia\": [\"awo baako ɛna wo kyere wo hunu se eii\", \"awo baako ɛna wo kyere wo hunu se eii\", \"awo ɛna baako wo kyere wo hunu sɛ eii\"],\n",
    "        \"john kuma paaaaaaaa funeral awwwwww ghana ne gyimie\": [\"john kuma paaaaaaaa ayie awwwwww ghana ne nkwasea\", \"ghana ne nkwaseafoɔ\", \"aww ayie\"],\n",
    "        \"yakubu sɛ preko yɛ haram nanso ɔdi trumu eiii\": [\"yakubu sɛ preko yɛ haram\", \"preko yɛ haram nanso ɔdi trumu\", \"ɔdi trumu eiii\"],\n",
    "        \"firi yɛn so kɔ\": [\"gyae yɛn\", \"firi yɛn so kɔ\", \"twe wo ho fi yɛn ho\"],\n",
    "        \"sika na shisika no ashi\": [\"sika no akɔ\", \"sika no ahyew\", \"sika no ahyew\"],\n",
    "        \"ohia kasa fa womuã na wo didi atem\": [\"ohia ka womuã ho na wo bu atɛn\", \"wo bu atɛn\", \"ohia ka womuã ho na wo bu atɛn\"],\n",
    "        \"ashawofo na ɛdi valentine\": [\"ashawofo na ɛdi valentine\", \"nguaman di valentine\", \"nguaman di afahyɛ\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "data = []\n",
    "for label, phrases in lexicon.items():\n",
    "    for canonical, variants in phrases.items():\n",
    "        data.append({\"tweet\": canonical, \"label\": label})\n",
    "        for v in variants:\n",
    "            data.append({\"tweet\": v, \"label\": label})\n",
    "\n",
    "lexicon_df = pd.DataFrame(data)\n",
    "\n",
    "combined = pd.read_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/datasets/combined_1.csv\")\n",
    "\n",
    "# Target size = original size + 60%\n",
    "target_size = int(len(combined) * 1.6)\n",
    "needed = target_size - len(combined)\n",
    "\n",
    "if needed > len(lexicon_df):\n",
    "    sampled = lexicon_df.sample(n=needed, replace=True, random_state=42)\n",
    "else:\n",
    "    sampled = lexicon_df.sample(n=needed, replace=False, random_state=42)\n",
    "\n",
    "# Merge\n",
    "augmented = pd.concat([combined, sampled], ignore_index=True)\n",
    "\n",
    "augmented.to_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/datasets/combined_2.csv\", index=False)\n",
    "print(f\"Augmentation complete: {len(combined)} → {len(augmented)} rows\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ab2d11-d7b1-42b4-b0bd-e26a4518c713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "negative    6188\n",
      "positive    5820\n",
      "neutral     5294\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Printing the count of all labels\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "augmented = pd.read_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/datasets/combined_2.csv\")\n",
    "print(augmented[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fec537-b59d-43b2-b7b8-6cead29d41ba",
   "metadata": {},
   "source": [
    "### Clean dataset (drop NaNs + drop bad labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8651d0b7-2ef7-492b-8de8-1292683d9831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset: 17302 rows kept\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/datasets/combined_2.csv\")\n",
    "\n",
    "df = df.dropna(subset=[\"tweet\", \"label\"])\n",
    "\n",
    "valid_labels = {\"neutral\", \"negative\", \"positive\"}\n",
    "df = df[df[\"label\"].str.lower().isin(valid_labels)]\n",
    "\n",
    "df.to_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/datasets/combined_3.csv\", index=False)\n",
    "\n",
    "print(f\"Cleaned dataset: {df.shape[0]} rows kept\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e832d80-cb19-4bc2-af70-70a7a8ee37df",
   "metadata": {},
   "source": [
    "### Split dataset into 80, 10 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9c4ec49-9d17-46a7-b1a9-43855484bbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 13841, Dev: 1730, Test: 1731\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/datasets/combined_3.csv\")\n",
    "\n",
    "train, temp = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "\n",
    "dev, test = train_test_split(temp, test_size=0.5, random_state=42, stratify=temp[\"label\"])\n",
    "\n",
    "train.to_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/datasets/train.csv\", index=False)\n",
    "dev.to_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/datasets/dev.csv\", index=False)\n",
    "test.to_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/datasets/test.csv\", index=False)\n",
    "\n",
    "print(f\"Train: {len(train)}, Dev: {len(dev)}, Test: {len(test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
