{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aafdf435-1d49-4604-86c4-93d49699e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.\tText Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aeccc90-0650-4d97-8eb0-b0fb5ebef56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet     label\n",
      "0                        kako be shark but wo ti ewu  negative\n",
      "1            br ne bayie nti na me supporti man city  negative\n",
      "2  s woofis mada wafutuo tantan no ywo smafa dabi...  negative\n",
      "3          wab…îdam anaa wo trumu y…õ nkate nkwan ase…õ  negative\n",
      "4                                  enfa bi da bra ü§£ü§£  negative\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "train_df = pd.read_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/Combined/combined_train.csv\")\n",
    "dev_df   = pd.read_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/Combined/combined_dev.csv\")\n",
    "test_df  = pd.read_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/Combined/combined_test.csv\")\n",
    "\n",
    "punct_to_remove = string.punctuation.replace(\"#\", \"\").replace(\"@\", \"\")\n",
    "# preserve # and @\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"  \n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = text.translate(str.maketrans('', '', punct_to_remove))  \n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text() \n",
    "    return text.strip()\n",
    "\n",
    "train_df[\"tweet\"] = train_df[\"tweet\"].apply(clean_text)\n",
    "dev_df[\"tweet\"]   = dev_df[\"tweet\"].apply(clean_text)\n",
    "test_df[\"tweet\"]  = test_df[\"tweet\"].apply(clean_text)\n",
    "\n",
    "\n",
    "output_folder = \"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/cleaned_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "train_df.to_csv(os.path.join(output_folder, \"cleaned_train.csv\"), index=False)\n",
    "dev_df.to_csv(os.path.join(output_folder, \"cleaned_dev.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(output_folder, \"cleaned_test.csv\"), index=False)\n",
    "\n",
    "\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f0316bb-d94a-42f7-a2a1-06fbc7c92d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.  Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdb29738-dfad-4df1-85d4-15c7cad266ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  \\\n",
      "0                        kako be shark but wo ti ewu   \n",
      "1            br ne bayie nti na me supporti man city   \n",
      "2  s woofis mada wafutuo tantan no ywo smafa dabi...   \n",
      "3          wab…îdam anaa wo trumu y…õ nkate nkwan ase…õ   \n",
      "4                                  enfa bi da bra ü§£ü§£   \n",
      "\n",
      "                                              tokens  \n",
      "0                [kako, be, shark, but, wo, ti, ewu]  \n",
      "1  [br, ne, bayie, nti, na, me, supporti, man, city]  \n",
      "2  [s, woofis, mada, wafutuo, tantan, no, ywo, sm...  \n",
      "3  [wab…îdam, anaa, wo, trumu, y…õ, nkate, nkwan, a...  \n",
      "4                          [enfa, bi, da, bra, ü§£, ü§£]  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "train_df = pd.read_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/cleaned_data/cleaned_train.csv\")\n",
    "dev_df   = pd.read_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/cleaned_data/cleaned_dev.csv\")\n",
    "test_df  = pd.read_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/cleaned_data/cleaned_test.csv\")\n",
    "\n",
    "def twi_tokenize(text):\n",
    "    \"\"\"\n",
    "    Custom tokenizer for Twi-English code-switched tweets.\n",
    "    Preserves Twi diacritics, emojis, hashtags, and @mentions.\n",
    "    \"\"\"\n",
    "    # Regex explanation:\n",
    "    # - @\\w+ ‚Üí mentions\n",
    "    # - #\\w+ ‚Üí hashtags\n",
    "    # - \\w+ ‚Üí words (keeps …õ, …î, etc.)\n",
    "    # - [^\\w\\s] ‚Üí emojis or special symbols\n",
    "    tokens = re.findall(r\"@\\w+|#\\w+|\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
    "    return tokens\n",
    "\n",
    "# Apply to datasets\n",
    "train_df[\"tokens\"] = train_df[\"tweet\"].apply(twi_tokenize)\n",
    "dev_df[\"tokens\"]   = dev_df[\"tweet\"].apply(twi_tokenize)\n",
    "test_df[\"tokens\"]  = test_df[\"tweet\"].apply(twi_tokenize)\n",
    "\n",
    "\n",
    "output_folder = \"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/tokenized_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "train_df.to_csv(os.path.join(output_folder, \"tokenized_train.csv\"), index=False)\n",
    "dev_df.to_csv(os.path.join(output_folder, \"tokenized_dev.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(output_folder, \"tokenized_test.csv\"), index=False)\n",
    "\n",
    "\n",
    "print(train_df[[\"tweet\", \"tokens\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2affc15d-688c-4f7f-9b3f-3c1f1ed1677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.\tStop Words Removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d8adca4-896c-403c-806b-effc90e85c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\T-Plug\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tokens  \\\n",
      "0                [kako, be, shark, but, wo, ti, ewu]   \n",
      "1  [br, ne, bayie, nti, na, me, supporti, man, city]   \n",
      "2  [s, woofis, mada, wafutuo, tantan, no, ywo, sm...   \n",
      "3  [wab…îdam, anaa, wo, trumu, y…õ, nkate, nkwan, a...   \n",
      "4                          [enfa, bi, da, bra, ü§£, ü§£]   \n",
      "\n",
      "                                     filtered_tokens  \n",
      "0                             [kako, shark, ti, ewu]  \n",
      "1              [br, bayie, nti, supporti, man, city]  \n",
      "2  [woofis, mada, wafutuo, tantan, ywo, smafa, da...  \n",
      "3         [wab…îdam, anaa, trumu, nkate, nkwan, ase…õ]  \n",
      "4                                  [enfa, bra, ü§£, ü§£]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download English stopwords if not already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# English + Twi stopwords\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "twi_stopwords = {\n",
    "    \"na\", \"ne\", \"y…õ\", \"wo\", \"de\", \"nso\", \"a\", \"…îno\", \"me\", \"mo\",\n",
    "    \"y…õn\", \"woara\", \"…înoa\", \"ene\", \"…õno\", \"w…în\", \"se\", \"di\", \"ni\",\n",
    "    \"no\", \"ka\", \"k…î\", \"ba\", \"y…õ…õ\", \"y…õ\", \"y?\", \"da\", \"re\", \"aa\",\n",
    "    \"w…î\", \"…õ\", \"mu\", \"ho\", \"bi\"\n",
    "}\n",
    "\n",
    "combined_stopwords = english_stopwords.union(twi_stopwords)\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/tokenized_data/tokenized_train.csv\")\n",
    "dev_df   = pd.read_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/tokenized_data/tokenized_dev.csv\")\n",
    "test_df  = pd.read_csv(\"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/tokenized_data/tokenized_test.csv\")\n",
    "\n",
    "# Fix: Convert stringified lists back to real lists\n",
    "for df in [train_df, dev_df, test_df]:\n",
    "    df[\"tokens\"] = df[\"tokens\"].apply(ast.literal_eval)\n",
    "\n",
    "# Apply stopword removal\n",
    "train_df[\"filtered_tokens\"] = train_df[\"tokens\"].apply(\n",
    "    lambda doc: [word for word in doc if word not in combined_stopwords]\n",
    ")\n",
    "dev_df[\"filtered_tokens\"] = dev_df[\"tokens\"].apply(\n",
    "    lambda doc: [word for word in doc if word not in combined_stopwords]\n",
    ")\n",
    "test_df[\"filtered_tokens\"] = test_df[\"tokens\"].apply(\n",
    "    lambda doc: [word for word in doc if word not in combined_stopwords]\n",
    ")\n",
    "\n",
    "# Save filtered datasets (with safer serialization of lists)\n",
    "output_folder = \"C:/Users/T-Plug/Desktop/University/Level 300/2nd Sem/316 Social Media Mining/End_Of_Sem_Project/Datasets/stop_words_removed\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "train_df.to_csv(os.path.join(output_folder, \"filtered_train.csv\"), index=False)\n",
    "dev_df.to_csv(os.path.join(output_folder, \"filtered_dev.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(output_folder, \"filtered_test.csv\"), index=False)\n",
    "\n",
    "# Quick check\n",
    "print(train_df[[\"tokens\", \"filtered_tokens\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a3c598d-3d15-4fb1-96f3-6d4d73cb25d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.\tStemming and Lemmatization \n",
    "# will be skipping as Twi has no official morphological analyzer in NLTK/spacy, \n",
    "# many papers just skip stemming/lemmatization and keep the filtered tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "218dfce7-9f15-4545-b8d9-782a611880c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.\tHandling Emojis and Emoticons \n",
    "# will be skipping demojizing for maximum performance, maximum performance ‚Üí keep raw emoji, since the most nlp models can handle emojis well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2879665e-7f56-4915-98c3-e2874c4f4a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6.\tSpell Checking \n",
    "#  Many Twi NLP pipelines skip spellchecking because of limited resources.\n",
    "#  Social media text in Twi often has slang, code-switching, intentional misspellings ‚Üí auto-correcting can damage meaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
